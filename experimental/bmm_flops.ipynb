{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2027e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def benchmark_bmm(b, m, n, k, num_iterations=100):\n",
    "    A = torch.randn((b, m, n)).half().to(\"cuda:0\")\n",
    "    B = torch.randn((b, n, k)).half().to(\"cuda:0\")\n",
    "    C = torch.empty((b, m, k)).half().to(\"cuda:0\")\n",
    "    num_warmup_iterations = 50\n",
    "    for i in range(num_warmup_iterations + num_iterations):\n",
    "        if i == num_warmup_iterations:\n",
    "            start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            torch.bmm(A, B, out=C)\n",
    "        torch.cuda.synchronize()\n",
    "    elapsed_time = (time.time() - start_time) / num_iterations\n",
    "    print(f\"Elapsed time for {b}x{m}x{n}x{k}: {elapsed_time:.3f}\")\n",
    "    print(f\"Throughput (in TFLOP/s) for {b}x{m}x{n}x{k}: {(2 * b * m * n * k) / (elapsed_time * 10**12):.3f}\")\n",
    "    flops = (2 * b * m * n * k) / (elapsed_time * 10**12)\n",
    "    print(\"-\" * 80)\n",
    "    return flops\n",
    "\n",
    "def benchmark_bmm_max(b, m, n, k, num_iterations=200):\n",
    "    A = torch.randn((b, m, n)).half().to(\"cuda:0\")\n",
    "    B = torch.randn((b, n, k)).half().to(\"cuda:0\")\n",
    "    C = torch.empty((b, m, k)).half().to(\"cuda:0\")\n",
    "    num_warmup_iterations=50\n",
    "    times = np.zeros(num_iterations+num_warmup_iterations)\n",
    "    start_time = time.time()\n",
    "    for i in range(num_warmup_iterations + num_iterations):\n",
    "        with torch.no_grad():\n",
    "            torch.bmm(A, B, out=C)\n",
    "        torch.cuda.synchronize()\n",
    "        times[i] = time.time()\n",
    "\n",
    "    #elapsed_time = (time.time() - start_time) / num_iterations\n",
    "    times -= start_time\n",
    "    times = np.diff(times)\n",
    "    times = times[50:]\n",
    "    elapsed_time = np.amax(times)\n",
    "    print(f\"Elapsed time for {b}x{m}x{n}x{k}: {elapsed_time:.3f}\")\n",
    "    print(f\"Throughput (in TFLOP/s) for {b}x{m}x{n}x{k}: {(2 * b * m * n * k) / (elapsed_time * 10**12):.3f}\")\n",
    "    flops = (2 * b * m * n * k) / (elapsed_time * 10**12)\n",
    "    print(\"-\" * 80)\n",
    "    return flops\n",
    "\n",
    "def benchmark_bmm_min(b, m, n, k, num_iterations=200):\n",
    "    A = torch.randn((b, m, n)).half().to(\"cuda:0\")\n",
    "    B = torch.randn((b, n, k)).half().to(\"cuda:0\")\n",
    "    C = torch.empty((b, m, k)).half().to(\"cuda:0\")\n",
    "    num_warmup_iterations=50\n",
    "    times = np.zeros(num_iterations+num_warmup_iterations)\n",
    "    start_time = time.time()\n",
    "    for i in range(num_warmup_iterations + num_iterations):\n",
    "        with torch.no_grad():\n",
    "            torch.bmm(A, B, out=C)\n",
    "        torch.cuda.synchronize()\n",
    "        times[i] = time.time()\n",
    "\n",
    "    #elapsed_time = (time.time() - start_time) / num_iterations\n",
    "    times -= start_time\n",
    "    times = np.diff(times)\n",
    "    times = times[50:]\n",
    "    elapsed_time = np.amin(times)\n",
    "    print(f\"Elapsed time for {b}x{m}x{n}x{k}: {elapsed_time:.3f}\")\n",
    "    print(f\"Throughput (in TFLOP/s) for {b}x{m}x{n}x{k}: {(2 * b * m * n * k) / (elapsed_time * 10**12):.3f}\")\n",
    "    flops = (2 * b * m * n * k) / (elapsed_time * 10**12)\n",
    "    print(\"-\" * 80)\n",
    "    return flops\n",
    "\n",
    "def bench_list(b, m, N, k):\n",
    "    benches = []\n",
    "    for n in N:\n",
    "        benches.append(benchmark_bmm(b, m, n, k))\n",
    "    return benches\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    torch.cuda.set_device(\"cuda:0\")\n",
    "\n",
    "    #shared dimension sweep.\n",
    "    #N_values= range(64, 2**12, 64)\n",
    "    #for logb in range(5, 9):\n",
    "    #    bench_list(b=2**logb, m=2048, N=N_values, k=2048)\n",
    "\n",
    "    # Try to determine the effect of b on throughput with square individual MMs.\n",
    "    # m n k 是矩阵大小 m x n * n x k = m x k\n",
    "    '''for log_b in range(7):\n",
    "        b = 2**log_b\n",
    "        benchmark_bmm(b, m=1024, n=1024, k=1024)\n",
    "        benchmark_bmm(b, m=2048, n=2048, k=2048)\n",
    "        benchmark_bmm(b, m=4096, n=4096, k=4096)\n",
    "        benchmark_bmm(b, m=8192, n=8192, k=8192)\n",
    "    '''\n",
    "    # Try to determine the effect of b and outer_dim on throughput with non-square\n",
    "    # individual MMs.\n",
    "    for log_b in range(7):\n",
    "        b = 2**log_b\n",
    "        for log_outer_dim in range(5, 14):\n",
    "            outer_dim = 2**log_outer_dim\n",
    "            benchmark_bmm_min(b, m=outer_dim, n=4096, k=outer_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
